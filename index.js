// Senti â€” Cloudflare Worker (Telegram bot) with memory + multimodel routing
// TEXT: Gemini -> DeepSeek -> Workers AI (AI binding or Gateway)
// VISION: Gemini -> Workers AI LLaVA
// Memory: Upstash Redis (last 10 pairs)

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const TG_API = (t) => `https://api.telegram.org/bot${t}`;
const TG_FILE = (t) => `https://api.telegram.org/file/bot${t}`;

const GEMINI_TEXT_MODEL = "gemini-1.5-flash";
const GEMINI_VISION_MODEL = "gemini-1.5-flash";
const TIMEOUT_GEMINI_MS = 5000;   // 5s -> switch to DeepSeek/fallback
const TIMEOUT_DEEPSEEK_MS = 6000; // 6s

const MEMORY_TURNS = 10; // keep last 10 user/assistant pairs

const SYSTEM_TEXT = "Ğ¢Ğ¸ â€” Senti: Ğ»Ğ°ĞºĞ¾Ğ½Ñ–Ñ‡Ğ½Ğ¸Ğ¹, Ğ´Ñ€ÑƒĞ¶Ğ½Ñ–Ğ¹, Ñ‚Ğ¾Ñ‡Ğ½Ğ¸Ğ¹. Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹ ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¾Ñ, ÑĞ¿Ğ¸ÑĞºĞ°Ğ¼Ğ¸ ĞºĞ¾Ğ»Ğ¸ Ğ´Ğ¾Ñ€ĞµÑ‡Ğ½Ğ¾.";
const SYSTEM_VISION = "Ğ¢Ğ¸ â€” Senti. ĞĞ¿Ğ¸ÑˆĞ¸ Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ¾Ğ´Ğ½Ñ–Ñ”Ñ Ñ„Ñ€Ğ°Ğ·Ğ¾Ñ, Ğ´Ğ°Ğ»Ñ– 3â€“5 ÑĞ¿Ğ¾ÑÑ‚ĞµÑ€ĞµĞ¶ĞµĞ½ÑŒ, Ğ¿Ğ¾Ñ‚Ñ–Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºÑ– Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºĞ¸.";

const CF_TEXT_FALLBACK_MODEL = "@cf/meta/llama-3.1-8b-instruct";
const CF_VISION_FALLBACK_MODEL = "@cf/llava-1.5-7b";

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UTILS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const json = (st, data) => new Response(JSON.stringify(data), {
  status: st,
  headers: { "content-type": "application/json; charset=utf-8" },
});
const nowISO = () => new Date().toISOString();
const b64 = (buf) => btoa(String.fromCharCode(...new Uint8Array(buf)));

async function withTimeout(promise, ms, label = "op") {
  const to = new Promise((_, rej) => setTimeout(() => rej(new Error(`${label}-timeout`)), ms));
  return Promise.race([promise, to]);
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ REDIS (Upstash) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function rGet(env, key) {
  const r = await fetch(`${env.REDIS_URL}/get/${encodeURIComponent(key)}`, {
    headers: { Authorization: `Bearer ${env.REDIS_TOKEN}` },
  });
  if (!r.ok) return null;
  const j = await r.json().catch(() => null);
  if (!j || j.result == null) return null;
  try { return JSON.parse(j.result); } catch { return j.result; }
}
async function rSet(env, key, val, ttlSec = 3 * 24 * 60 * 60) {
  const r = await fetch(`${env.REDIS_URL}/set/${encodeURIComponent(key)}?EX=${ttlSec}`, {
    method: "POST",
    headers: { Authorization: `Bearer ${env.REDIS_TOKEN}`, "content-type": "application/json" },
    body: JSON.stringify(val),
  });
  if (!r.ok) throw new Error(`redis-set ${r.status}`);
}
async function loadHist(env, chatId) {
  return (await rGet(env, `chat:${chatId}:hist`)) || [];
}
async function pushHist(env, chatId, role, content) {
  const key = `chat:${chatId}:hist`;
  const arr = await loadHist(env, chatId);
  arr.push({ role, content, ts: nowISO() });
  while (arr.length > MEMORY_TURNS * 2) arr.shift();
  await rSet(env, key, arr);
  return arr;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TELEGRAM API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function tgAction(env, chatId, action = "typing") {
  await fetch(`${TG_API(env.TELEGRAM_TOKEN)}/sendChatAction`, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify({ chat_id: chatId, action }),
  }).catch(() => {});
}
async function tgSend(env, chatId, text, replyTo) {
  return fetch(`${TG_API(env.TELEGRAM_TOKEN)}/sendMessage`, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify({
      chat_id: chatId,
      text,
      parse_mode: "Markdown",
      reply_to_message_id: replyTo,
      disable_web_page_preview: true,
    }),
  });
}
async function tgEdit(env, chatId, messageId, text) {
  return fetch(`${TG_API(env.TELEGRAM_TOKEN)}/editMessageText`, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify({
      chat_id: chatId,
      message_id: messageId,
      text,
      parse_mode: "Markdown",
      disable_web_page_preview: true,
    }),
  });
}
async function tgGetFileUrl(env, fileId) {
  const meta = await fetch(`${TG_API(env.TELEGRAM_TOKEN)}/getFile?file_id=${fileId}`).then(r => r.json());
  if (!meta?.ok) throw new Error("getFile failed");
  return `${TG_FILE(env.TELEGRAM_TOKEN)}/${meta.result.file_path}`;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI: GEMINI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function geminiText(env, messages) {
  if (!env.GEMINI_API_KEY) throw new Error("gemini-no-key");
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_TEXT_MODEL}:generateContent?key=${encodeURIComponent(env.GEMINI_API_KEY)}`;

  const contents = messages.map(m => ({
    role: m.role === "system" ? "user" : m.role, // Gemini Ğ½Ğµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” 'system' Ñƒ contents
    parts: [{ text: m.content }],
  }));

  const body = {
    system_instruction: { role: "system", parts: [{ text: SYSTEM_TEXT }] },
    contents,
  };

  const r = await fetch(url, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify(body),
  });
  if (!r.ok) throw new Error(`gemini-http-${r.status}`);
  const j = await r.json();
  const parts = j?.candidates?.[0]?.content?.parts || [];
  const text = parts.map(p => p?.text || "").join("").trim();
  if (!text) throw new Error("gemini-empty");
  return text;
}

async function geminiVision(env, prompt, imageBytes, mime = "image/jpeg") {
  if (!env.GEMINI_API_KEY) throw new Error("gemini-no-key");
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${GEMINI_VISION_MODEL}:generateContent?key=${encodeURIComponent(env.GEMINI_API_KEY)}`;

  const body = {
    system_instruction: { role: "system", parts: [{ text: SYSTEM_VISION }] },
    contents: [{
      role: "user",
      parts: [
        { text: (prompt || "").trim() || "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ¹ Ñ„Ğ¾Ñ‚Ğ¾ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ñ‰Ğµ." },
        { inline_data: { mime_type: mime, data: b64(imageBytes) } },
      ],
    }],
  };

  const r = await fetch(url, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify(body),
  });
  if (!r.ok) throw new Error(`gemini-vis-http-${r.status}`);
  const j = await r.json();
  const parts = j?.candidates?.[0]?.content?.parts || [];
  const text = parts.map(p => p?.text || "").join("").trim();
  if (!text) throw new Error("gemini-vis-empty");
  return text;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI: DEEPSEEK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function deepseekText(env, messages) {
  if (!env.DEEPSEEK_API_KEY) throw new Error("deepseek-no-key");
  const url = "https://api.deepseek.com/chat/completions";
  const body = {
    model: "deepseek-chat",
    messages: messages.map(m => ({ role: m.role === "system" ? "system" : m.role, content: m.content })),
    stream: false,
  };
  const r = await fetch(url, {
    method: "POST",
    headers: { "content-type": "application/json", Authorization: `Bearer ${env.DEEPSEEK_API_KEY}` },
    body: JSON.stringify(body),
  });
  if (!r.ok) throw new Error(`deepseek-http-${r.status}`);
  const j = await r.json();
  const out = j?.choices?.[0]?.message?.content?.trim();
  if (!out) throw new Error("deepseek-empty");
  return out;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Workers AI fallback (AI or Gateway) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function aiGateway(env, model, payload) {
  if (!env.CF_AI_GATEWAY_BASE) throw new Error("no-gateway");
  const url = `${env.CF_AI_GATEWAY_BASE}/workers-ai/${encodeURIComponent(model)}`;
  const r = await fetch(url, {
    method: "POST",
    headers: {
      "content-type": "application/json",
      ...(env.CF_API_TOKEN ? { Authorization: `Bearer ${env.CF_API_TOKEN}` } : {}),
    },
    body: JSON.stringify(payload),
  });
  if (!r.ok) throw new Error(`gateway-${model}-${r.status}`);
  return r.json();
}

async function workersTextFallback(env, messages) {
  // 1) Prefer local binding (faster)
  if (env.AI) {
    const res = await env.AI.run(CF_TEXT_FALLBACK_MODEL, { messages });
    const out = res?.response || res?.result || res?.output_text || "";
    if (out) return String(out).trim();
  }
  // 2) Fallback via Gateway (if configured)
  const j = await aiGateway(env, CF_TEXT_FALLBACK_MODEL, { messages });
  const out = j?.response || j?.result || j?.output_text || j?.choices?.[0]?.message?.content || "";
  if (!out) throw new Error("workers-text-empty");
  return String(out).trim();
}

async function workersVisionFallback(env, prompt, imageBytes) {
  const content = [
    { type: "input_text", text: (prompt || "ĞĞ¿Ğ¸ÑˆĞ¸ Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¾Ñ.") },
    { type: "input_image", image: b64(imageBytes) },
  ];
  if (env.AI) {
    const res = await env.AI.run(CF_VISION_FALLBACK_MODEL, {
      messages: [{ role: "user", content }],
    });
    const out = res?.response || res?.result || res?.output_text || "";
    if (out) return String(out).trim();
  }
  const j = await aiGateway(env, CF_VISION_FALLBACK_MODEL, {
    messages: [{ role: "user", content }],
  });
  const out = j?.response || j?.result || j?.output_text || j?.choices?.[0]?.message?.content || "";
  if (!out) throw new Error("workers-vis-empty");
  return String(out).trim();
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ High-level: askText/askVision with fallbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function askTextLLM(env, messages) {
  // 1) Gemini (5s)
  try {
    return await withTimeout(geminiText(env, messages), TIMEOUT_GEMINI_MS, "gemini-text");
  } catch (_) {}
  // 2) DeepSeek (6s)
  try {
    return await withTimeout(deepseekText(env, messages), TIMEOUT_DEEPSEEK_MS, "deepseek-text");
  } catch (_) {}
  // 3) Workers AI (final fallback)
  try {
    return await workersTextFallback(env, messages);
  } catch (_) {}
  return "Ğ’Ğ¸Ğ±Ğ°Ñ‡, Ñ Ñ‚Ğ¸Ğ¼Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¹. Ğ¡Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹ Ñ‰Ğµ Ñ€Ğ°Ğ· Ğ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ.";
}

async function askVisionLLM(env, prompt, imageBytes) {
  // 1) Gemini Vision
  try {
    return await withTimeout(geminiVision(env, prompt, imageBytes), TIMEOUT_GEMINI_MS, "gemini-vision");
  } catch (_) {}
  // 2) Workers AI LLaVA (final)
  try {
    return await workersVisionFallback(env, prompt, imageBytes);
  } catch (_) {}
  return "Ğ¯ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ² Ñ„Ğ¾Ñ‚Ğ¾, Ğ°Ğ»Ğµ Ğ½Ğ°Ñ€Ğ°Ğ·Ñ– Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ñƒ Ğ¹Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ·Ğ¿Ñ–Ğ·Ğ½Ğ°Ñ‚Ğ¸. Ğ¡Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹, Ğ±ÑƒĞ´ÑŒ Ğ»Ğ°ÑĞºĞ°, Ñ‰Ğµ Ñ€Ğ°Ğ·.";
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Telegram update handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function handleUpdate(env, update) {
  const msg = update.message || update.edited_message;
  if (!msg) return;

  const chatId = msg.chat.id;
  const replyTo = msg.message_id;

  // typingâ€¦
  tgAction(env, chatId, "typing").catch(() => {});

  // /start â€” Ğ¶Ğ¸Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ²Ñ–Ñ‚Ğ°Ğ½Ğ½Ñ (6 Ğ²Ğ°Ñ€Ñ–Ğ°Ğ½Ñ‚Ñ–Ğ² + Ñ–Ğ¼'Ñ)
  if (msg.text && /^\/start\b/i.test(msg.text)) {
    const firstName = msg.from?.first_name || msg.from?.username || "Ğ´Ñ€ÑƒĞ¶Ğµ";
    const variants = [
      `ĞŸÑ€Ğ¸Ğ²Ñ–Ñ‚, ${firstName}! ğŸš€ Ğ”Ğ°Ğ²Ğ°Ğ¹ Ğ·Ñ€Ğ¾Ğ±Ğ¸Ğ¼Ğ¾ Ñ†ĞµĞ¹ Ğ´ĞµĞ½ÑŒ Ñ‚Ñ€Ğ¾ÑˆĞºĞ¸ ÑÑĞºÑ€Ğ°Ğ²Ñ–ÑˆĞ¸Ğ¼.`,
      `Ğ¥ĞµĞ¹, ${firstName}! ğŸ˜ Ğ¯ Senti. Ğ©Ğ¾ Ğ¿Ñ–Ğ´ĞºĞ¸Ğ½ÑƒÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾?`,
      `Ğ Ğ°Ğ´Ğ¸Ğ¹ Ğ±Ğ°Ñ‡Ğ¸Ñ‚Ğ¸, ${firstName}! âœ¨ ĞŸĞ¸ÑˆĞ¸, Ğ· Ñ‡Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‡Ğ½ĞµĞ¼Ğ¾.`,
      `ĞŸÑ€Ğ¸Ğ²Ñ–Ñ‚-Ğ¿Ñ€Ğ¸Ğ²Ñ–Ñ‚, ${firstName}! ğŸ™Œ Ğ¯ Ğ¿Ğ¾Ñ€ÑƒÑ‡. Ğ¢ĞµĞºÑÑ‚ Ñ‡Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾ â€” Ñ– Ğ¿Ğ¾Ñ—Ñ…Ğ°Ğ»Ğ¸.`,
      `${firstName}, Ğ¿Ñ€Ğ¸Ğ²Ñ–Ñ‚! ğŸ§  Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ñ‚Ğ¸. Ğ©Ğ¾ Ğ² Ğ¿Ñ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ñ–?`,
      `Ğ’Ñ–Ñ‚Ğ°Ñ, ${firstName}! ğŸ”§ Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ğ»Ğ°Ğ½ Ğ°Ğ±Ğ¾ Ñ€Ğ¾Ğ·Ğ±ĞµÑ€ĞµĞ¼Ğ¾ Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ?`,
    ];
    const hi = variants[Math.floor(Math.random() * variants.length)]
      + `\n\nâ€¢ ĞĞ°Ğ´Ñ–ÑˆĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ â€” Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ¼ Ğ»Ğ°ĞºĞ¾Ğ½Ñ–Ñ‡Ğ½Ğ¾.\nâ€¢ ĞŸÑ€Ğ¸ÑˆĞ»Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾ â€” Ğ¾Ğ¿Ğ¸ÑˆÑƒ Ñ‚Ğ° Ğ´Ğ°Ğ¼ Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºĞ¸.`;
    await tgSend(env, chatId, hi, replyTo);
    return;
  }

  // Photo
  if (msg.photo && msg.photo.length) {
    const best = msg.photo[msg.photo.length - 1];
    try {
      const fileUrl = await tgGetFileUrl(env, best.file_id);
      const imgResp = await fetch(fileUrl);
      const bytes = new Uint8Array(await imgResp.arrayBuffer());
      const prompt = (msg.caption || "").trim();

      const hist = await loadHist(env, chatId);
      const sys = "Ğ¢Ğ¸ â€” Senti. ĞĞ¿Ğ¸ÑˆĞ¸ Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ ÑÑ‚Ğ¸ÑĞ»Ğ¾, Ğ¿Ğ¾Ñ‚Ñ–Ğ¼ 2â€“4 Ğ²Ğ¸ÑĞ½Ğ¾Ğ²ĞºĞ¸ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼.";
      const answer = await askVisionLLM(env, `${sys}\n\n${prompt}`, bytes);

      await pushHist(env, chatId, "user", "[image]" + (prompt ? ` ${prompt}` : ""));
      await pushHist(env, chatId, "assistant", answer);
      await tgSend(env, chatId, answer, replyTo);
    } catch {
      await tgSend(env, chatId, "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾ ğŸ˜…", replyTo);
    }
    return;
  }

  // Text
  if (msg.text) {
    const userText = msg.text.trim();

    // Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ğ¹ Ğ³Ğ°Ñ€Ğ´ Ğ½Ğ° live-Ğ´Ğ°Ğ½Ñ–
    if (/ĞºÑƒÑ€Ñ\s+Ğ´Ğ¾Ğ»Ğ°Ñ€Ğ°/i.test(userText)) {
      await tgSend(env, chatId, "Ğ¯ Ğ½Ğµ Ğ¼Ğ°Ñ live-Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ñƒ Ğ´Ğ¾ ĞºÑƒÑ€ÑÑ–Ğ². ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ Ñƒ Ğ±Ğ°Ğ½ĞºÑƒ Ğ°Ğ±Ğ¾ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ñ– ĞĞ‘Ğ£.", replyTo);
      return;
    }

    const hist = await loadHist(env, chatId);
    const system = "Ğ¢Ğ¸ â€” Senti, Ğ´Ñ€ÑƒĞ¶Ğ½Ñ–Ğ¹ Ñ– Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¸Ğ¹ Ğ°ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹ ÑÑ‚Ğ¸ÑĞ»Ğ¾, ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¾Ñ, Ğ¿Ğ¾ ÑÑƒÑ‚Ñ–. Ğ”Ğµ Ğ´Ğ¾Ñ€ĞµÑ‡Ğ½Ğ¾ â€” 3â€“5 Ğ±ÑƒĞ»Ñ–Ñ‚Ñ–Ğ².";
    const messages = [
      { role: "system", content: system },
      ...hist.map(h => ({ role: h.role, content: h.content })),
      { role: "user", content: userText },
    ];

    const answer = await askTextLLM(env, messages);
    await pushHist(env, chatId, "user", userText);
    await pushHist(env, chatId, "assistant", answer);
    await tgSend(env, chatId, answer, replyTo);
    return;
  }

  await tgSend(env, chatId, "ĞĞ°Ğ´Ñ–ÑˆĞ»Ğ¸, Ğ±ÑƒĞ´ÑŒ Ğ»Ğ°ÑĞºĞ°, Ñ‚ĞµĞºÑÑ‚ Ğ°Ğ±Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾ ğŸ™Œ", replyTo);
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export default {
  async fetch(request, env) {
    const { pathname } = new URL(request.url);

    // Health
    if (request.method === "GET" && (pathname === "/" || pathname === "/health")) {
      return json(200, { ok: true, service: "senti", time: nowISO() });
    }

    // Telegram webhook (any path) with secret header
    if (request.method === "POST") {
      const sec = request.headers.get("X-Telegram-Bot-Api-Secret-Token");
      if (!sec || sec !== env.WEBHOOK_SECRET) return json(401, { ok: false, error: "unauthorized" });

      let update;
      try { update = await request.json(); } catch { return json(400, { ok: false, error: "bad json" }); }

      try { await handleUpdate(env, update); } catch (_) {}
      return json(200, { ok: true });
    }

    return json(404, { ok: false, error: "not found" });
  },
};